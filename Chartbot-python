import os
import re
import json
import spacy
import requests  # Import requests for making HTTP calls
from flask import Flask, render_template, request, jsonify
from extract_msg import Message  # For reading .msg files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load the spaCy model
nlp = spacy.load("en_core_web_md")

# Create a Flask app
app = Flask(__name__)

# Path to folder containing .msg files
EMAIL_FOLDER = 'c:/Users/x01358215/SampleEmails'  # Replace with your folder path
KNOWLEDGE_BASE_FILE = 'knowledge.json'

# Global variable to store knowledge base (summarized emails)
knowledge_base = []

def clean_email_body(body):
    """Clean email body by removing common signatures, email chains, and unnecessary phrases."""
    body = body.replace('\r\n', '\n').replace('\r', '\n')
    body = re.sub(r'(On\s.*\n?.*wrote:)', '', body)
    body = re.sub(r'(From:.*\n)', '', body)
    body = re.sub(r'(Sent:.*\n)', '', body)
    body = re.sub(r'(To:.*\n)', '', body)
    body = re.sub(r'(Cc:.*\n)', '', body)
    body = re.sub(r'(Bcc:.*\n)', '', body)

    # Remove signature lines
    signature_patterns = [
        r'Best regards,',
        r'Thanks,',
        r'Sent from my iPhone',
        r'Sincerely,',
        r'Regards,',
        r'Cheers,'
    ]
    
    for pattern in signature_patterns:
        body = re.sub(pattern, '', body, flags=re.IGNORECASE)

    # Remove "RESTRICTED - INTERNAL" and similar classifications
    body = re.sub(r'RESTRICTED\s+-\s+INTERNAL', '', body, flags=re.IGNORECASE)
    body = re.sub(r'Caution:\s?.*', '', body, flags=re.IGNORECASE)  # Remove Caution statements

    # Remove extra white spaces and newlines
    body = re.sub(r'\n+', '\n', body).strip()

    return body

def extract_email_content(msg_file_path):
    """Extract subject, sender, receiver, cleaned body, and time from .msg file."""
    msg = Message(msg_file_path)
    subject = msg.subject
    sender = msg.sender
    receivers = msg.to  # Extract the receiver(s)
    body = clean_email_body(msg.body)
    email_time = msg.date.strftime("%Y-%m-%d %H:%M:%S") if msg.date else "Unknown"
    return subject, sender, receivers, body, email_time

def clean_summary_text(text):
    """Clean summary text to remove unwanted phrases."""
    # Remove unwanted phrases
    text = re.sub(r'Subject:\s?.*', '', text)
    text = re.sub(r'RESTRICTED\s+-\s+INTERNAL', '', text, flags=re.IGNORECASE)
    text = re.sub(r'Caution:\s?.*', '', text, flags=re.IGNORECASE)
    return text.strip()

def summarize_text_spacy(text):
    """Summarize email content using spaCy."""
    cleaned_text = clean_summary_text(text)  # Clean the text
    doc = nlp(cleaned_text)
    sentences = [sent.text for sent in doc.sents if len(sent.text.split()) > 5]
    return ' '.join(sentences[:3])  # Use first 3 sentences as summary

def summarize_with_keywords(text):
    """Extract keywords as a form of summarization."""
    cleaned_text = clean_summary_text(text)  # Clean the text
    doc = nlp(cleaned_text)
    keywords = [token.text for token in doc if token.is_keyword or token.is_stop]
    return ' '.join(set(keywords))

def summarize_with_pos_weighting(text):
    """Summarize text with POS weighting."""
    cleaned_text = clean_summary_text(text)  # Clean the text
    doc = nlp(cleaned_text)
    sentences = [sent.text for sent in doc.sents if len(sent.text.split()) > 5]
    
    # Weight sentences based on the number of nouns and verbs
    sentence_scores = []
    for sentence in sentences:
        score = sum(1 for token in nlp(sentence) if token.pos_ in ['NOUN', 'VERB'])
        sentence_scores.append((sentence, score))

    # Sort sentences based on scores
    sentence_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)
    return ' '.join([sentence for sentence, score in sentence_scores[:3]])  # Top 3 sentences

def summarize_with_dependency_parsing(text):
    """Summarize text using dependency parsing."""
    cleaned_text = clean_summary_text(text)  # Clean the text
    doc = nlp(cleaned_text)
    # Extract the root of the sentence to create a summary
    root_sentences = [sent.root.text for sent in doc.sents if sent.root]
    return ' '.join(root_sentences)

def summarize_with_entities(text):
    """Summarize text using named entity recognition."""
    cleaned_text = clean_summary_text(text)  # Clean the text
    doc = nlp(cleaned_text)
    entities = [ent.text for ent in doc.ents]
    return ' '.join(set(entities))

def build_knowledge_base():
    """Read all .msg files and build the knowledge base."""
    global knowledge_base
    id_counter = 1  # Initialize ID counter
    for filename in os.listdir(EMAIL_FOLDER):
        if filename.endswith(".msg"):
            filepath = os.path.join(EMAIL_FOLDER, filename)
            subject, sender, receivers, body, time = extract_email_content(filepath)
            knowledge_base.append({
                'id': id_counter,  # Add ID to the entry
                'filename': filename,
                'subject': subject,
                'sender': sender,
                'receivers': receivers,  # Store receivers information
                'time': time,
                'summary_spacy': summarize_text_spacy(body),
                'summary_keywords': summarize_with_keywords(body),
                'summary_pos_weighting': summarize_with_pos_weighting(body),
                'summary_dependency': summarize_with_dependency_parsing(body),
                'summary_entities': summarize_with_entities(body),
                'body': body  # Save full body for detailed responses
            })
            id_counter += 1  # Increment the ID counter

def save_knowledge_base_to_json():
    """Save the knowledge base to a JSON file."""
    with open(KNOWLEDGE_BASE_FILE, 'w') as json_file:
        json.dump(knowledge_base, json_file, indent=4)
    print(f"Knowledge base saved to {KNOWLEDGE_BASE_FILE}")

def post_knowledge_base(url):
    """Post the knowledge base to a specified URL."""
    response = requests.post(url, json=knowledge_base)
    if response.status_code == 200:
        print("Successfully posted knowledge base.")
    else:
        print(f"Failed to post knowledge base. Status code: {response.status_code}")

def search_knowledge_base(query):
    """Search the knowledge base for relevant information based on the query using TF-IDF."""
    responses = []
    documents = [entry['summary_spacy'] + " " + entry['subject'] for entry in knowledge_base]

    # Create TF-IDF vectorizer
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents + [query])  # Add query to the documents for comparison

    # Calculate cosine similarity
    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()
    relevant_indices = cosine_similarities.argsort()[::-1]  # Sort in descending order

    for idx in relevant_indices:
        if cosine_similarities[idx] > 0:  # Only consider non-zero similarity scores
            responses.append({
                'id': knowledge_base[idx]['id'],  # Include ID in the response
                'subject': knowledge_base[idx]['subject'],
                'summary_spacy': knowledge_base[idx]['summary_spacy'],
                'summary_keywords': knowledge_base[idx]['summary_keywords'],
                'summary_pos_weighting': knowledge_base[idx]['summary_pos_weighting'],
                'summary_dependency': knowledge_base[idx]['summary_dependency'],
                'summary_entities': knowledge_base[idx]['summary_entities'],
                'body': knowledge_base[idx]['body'],
                'receivers': knowledge_base[idx]['receivers'],  # Include receivers information
                'time': knowledge_base[idx]['time']  # Include the email time
            })

    return responses

@app.route('/')
def index():
    """Home page for chatbot interaction."""
    return render_template('chat.html')

@app.route('/query', methods=['POST'])
def query():
    """API endpoint to handle chatbot queries."""
    user_query = request.form['query']
    results = search_knowledge_base(user_query)

    if results:
        return jsonify({
            'status': 'success',
            'data': results
        })
    else:
        return jsonify({
            'status': 'fail',
            'message': 'No relevant information found in the knowledge base.'
        })

if __name__ == '__
