import os
import re
import json
import spacy
from flask import Flask, render_template, request, jsonify
from extract_msg import Message  # For reading .msg files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load the spaCy model (assumed to be installed and available)
nlp = spacy.load("en_core_web_md")

# Create a Flask app
app = Flask(__name__)

# Path to folder containing .msg files
EMAIL_FOLDER = 'c:/Users/x01358215/SampleEmails'  # Replace with your folder path
KNOWLEDGE_BASE_FILE = 'knowledge.json'

# Global variable to store knowledge base (summarized emails)
knowledge_base = []

def clean_email_body(body):
    """
    Clean email body by removing common signatures, email chains,
    and unnecessary phrases.
    """
    # 1. Normalize line endings
    body = body.replace('\r\n', '\n').replace('\r', '\n')

    # 2. Remove email chains (e.g., "On [date], [sender] wrote:")
    body = re.sub(r'(On\s.*\n?.*wrote:)', '', body)
    body = re.sub(r'(From:.*\n)', '', body)
    body = re.sub(r'(Sent:.*\n)', '', body)
    body = re.sub(r'(To:.*\n)', '', body)
    body = re.sub(r'(Cc:.*\n)', '', body)
    body = re.sub(r'(Bcc:.*\n)', '', body)

    # 3. Remove signature lines (e.g., "Best regards", "Thanks", etc.)
    signature_patterns = [
        r'Best regards,',
        r'Thanks,',
        r'Sent from my iPhone',
        r'Sincerely,',
        r'Regards,',
        r'Cheers,'
    ]
    
    for pattern in signature_patterns:
        body = re.sub(pattern, '', body, flags=re.IGNORECASE)

    # 4. Remove extra white spaces and newlines
    body = re.sub(r'\n+', '\n', body).strip()

    # 5. Remove "RESTRICTED - INTERNAL" and similar classifications
    body = re.sub(r'RESTRICTED\s+-\s+INTERNAL', '', body, flags=re.IGNORECASE)

    return body

def extract_email_content(msg_file_path):
    """Extract subject, sender, cleaned body, and time from .msg file"""
    msg = Message(msg_file_path)
    subject = msg.subject
    sender = msg.sender
    body = clean_email_body(msg.body)
    # Extract the email's time (msg.date) and format it as a string
    email_time = msg.date.strftime("%Y-%m-%d %H:%M:%S") if msg.date else "Unknown"
    return subject, sender, body, email_time

def summarize_text(text, subject):
    """
    Summarize email content using spaCy and clean up any 'Subject:' lines or keywords.
    """
    # Remove "Subject: ..." if present in the body
    text = re.sub(r'Subject:\s?.*', '', text)

    # Remove unwanted keywords like "RESTRICTED - INTERNAL"
    text = re.sub(r'RESTRICTED\s+-\s+INTERNAL', '', text, flags=re.IGNORECASE)

    # Process with spaCy
    doc = nlp(text)
    
    # Extract sentences for summary
    sentences = [sent.text for sent in doc.sents if len(sent.text.split()) > 5]
    
    # Combine first 3 sentences as summary
    summary = ' '.join(sentences[:3])

    # Ensure subject isn't repeated in the summary
    if subject in summary:
        summary = summary.replace(subject, '')

    return summary.strip()

def build_knowledge_base():
    """Read all .msg files and build the knowledge base"""
    global knowledge_base
    for filename in os.listdir(EMAIL_FOLDER):
        if filename.endswith(".msg"):
            filepath = os.path.join(EMAIL_FOLDER, filename)
            subject, sender, body, time = extract_email_content(filepath)
            summary = summarize_text(body, subject)  # Pass both body and subject
            knowledge_base.append({
                'filename': filename,
                'subject': subject,
                'sender': sender,
                'summary': summary,
                'body': body,  # Save full body for detailed responses
                'time': time   # Save email sent time
            })

def save_knowledge_base_to_json():
    """Save the knowledge base to a JSON file"""
    with open(KNOWLEDGE_BASE_FILE, 'w') as json_file:
        json.dump(knowledge_base, json_file, indent=4)
    print(f"Knowledge base saved to {KNOWLEDGE_BASE_FILE}")

def search_knowledge_base(query):
    """Search the knowledge base for relevant information based on the query using TF-IDF"""
    responses = []

    # Prepare documents (summaries and subjects)
    documents = [entry['summary'] + " " + entry['subject'] for entry in knowledge_base]

    # Create TF-IDF vectorizer
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(documents + [query])  # Add query to the documents for comparison

    # Calculate cosine similarity
    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()

    # Get the indices of the most relevant documents
    relevant_indices = cosine_similarities.argsort()[::-1]  # Sort in descending order

    for idx in relevant_indices:
        if cosine_similarities[idx] > 0:  # Only consider non-zero similarity scores
            responses.append({
                'subject': knowledge_base[idx]['subject'],
                'summary': knowledge_base[idx]['summary'],
                'body': knowledge_base[idx]['body'],  # Save full body for detailed responses
                'time': knowledge_base[idx]['time']   # Include the email time
            })

    return responses

@app.route('/')
def index():
    """Home page for chatbot interaction"""
    return render_template('chat.html')

@app.route('/query', methods=['POST'])
def query():
    """API endpoint to handle chatbot queries"""
    user_query = request.form['query']
    results = search_knowledge_base(user_query)
    
    if results:
        return jsonify({
            'status': 'success',
            'data': results
        })
    else:
        return jsonify({
            'status': 'fail',
            'message': 'No relevant information found in the knowledge base.'
        })

if __name__ == '__main__':
    build_knowledge_base()  # Build the knowledge base when app starts
    save_knowledge_base_to_json()  # Save to JSON file
    app.run(debug=True)
